{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQuVY5kNjX5N"
      },
      "source": [
        "\n",
        "#\n",
        "# MODULO 07 - EJERCICIO 04-B\n",
        "# ALEXIS YURI M.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6GyNyFD-1dd"
      },
      "source": [
        "Primero es necesario instalar y configurar PySpark en el notebook de Colab. Para eso se ejecutan las siguientes celdas para instalar las librerías necesarias y crear un contexto de Spark.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXX0LdusjgpU",
        "outputId": "da667858-fc59-4a40-fae4-9599812d490f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "SparkSession está lista para usar.\n"
          ]
        }
      ],
      "source": [
        "# Se instala PySpark y findspark\n",
        "!pip install pyspark findspark\n",
        "\n",
        "# Se inicializa findspark para encontrar la instalación de Spark.\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Se importa SparkContext.\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Se crea una sesión de Spark si no existe.\n",
        "try:\n",
        "    spark.stop()\n",
        "except:\n",
        "    pass\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Ejemplo_UDF\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(\"SparkSession está lista para usar.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 1: Se crea un DataFrame desde una lista de tuplas.\n"
      ],
      "metadata": {
        "id": "YjmOAJsJkRNR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NGblWDt5jsHk"
      },
      "outputs": [],
      "source": [
        "# Paso 1: Se crea una lista de tuplas.\n",
        "\n",
        "data = [(\"Hugo\", 550000),\n",
        "        (\"Paco\", 800000),\n",
        "        (\"Luis\", 350000),\n",
        "        (\"Donald\", 1200000)]\n",
        "\n",
        "# Se define el esquema del DataFrame.\n",
        "schema = [\"nombre\", \"salario\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf4LTmNVjyzc"
      },
      "source": [
        "Paso 2: Convierte esa lista en un DataFrame usando spark.createDataFrame().\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GeX_Rs9Kj4xF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d4f8be-5681-4abf-942b-251fcfafa775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame original:\n",
            "+------+-------+\n",
            "|nombre|salario|\n",
            "+------+-------+\n",
            "|  Hugo| 550000|\n",
            "|  Paco| 800000|\n",
            "|  Luis| 350000|\n",
            "|Donald|1200000|\n",
            "+------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Se convierte la lista en un DataFrame.\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Se muestra el DataFrame.\n",
        "print(\"DataFrame original:\")\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 3: Se define una funcion en Python para categorizar salarios (alto/medio/bajo).\n"
      ],
      "metadata": {
        "id": "ghezmLjh0JPy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c2ppI_t90fgl"
      },
      "outputs": [],
      "source": [
        "# Se define una función en Python para categorizar los salarios.\n",
        "def categorizar_salario(salario):\n",
        "    if salario > 1000000:\n",
        "        return \"Alto\"\n",
        "    elif salario >= 500000:\n",
        "        return \"Medio\"\n",
        "    else:\n",
        "        return \"Bajo\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph73fqUqj_NR"
      },
      "source": [
        "\n",
        "Paso 4: Se registra la funcion como UDF."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se importan los módulos necesarios para la creación de la UDF.\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Se registra la función como UDF, especificando el tipo de dato de retorno.\n",
        "categorizar_salario_udf = udf(categorizar_salario, StringType())\n",
        "\n",
        "print(\"Función UDF registrada con éxito.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1rtWg1pNbMs",
        "outputId": "fdd8fce4-1c73-4ae3-e662-027ed66c754d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Función UDF registrada con éxito.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 5: Aplicar la UDF usando .withColumn()"
      ],
      "metadata": {
        "id": "lNY6f3DGo9tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se aplica la UDF.\n",
        "df_con_categoria = df.withColumn(\"categoria_salario\", categorizar_salario_udf(\"salario\"))"
      ],
      "metadata": {
        "id": "fQxPvRJgpBhO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 6: Mostrar los resultados.\n"
      ],
      "metadata": {
        "id": "p5F7iCf0pX5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se muestra el DataFrame con la nueva columna \"categoria salario\".\n",
        "print(\"DataFrame con la nueva columna 'categoria_salario':\")\n",
        "df_con_categoria.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aexIQprrpc_u",
        "outputId": "d58c1bb8-e3a4-474b-fa2a-e89664627052"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame con la nueva columna 'categoria_salario':\n",
            "+------+-------+-----------------+\n",
            "|nombre|salario|categoria_salario|\n",
            "+------+-------+-----------------+\n",
            "|  Hugo| 550000|            Medio|\n",
            "|  Paco| 800000|            Medio|\n",
            "|  Luis| 350000|             Bajo|\n",
            "|Donald|1200000|             Alto|\n",
            "+------+-------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se cierra Sesión"
      ],
      "metadata": {
        "id": "gUCGdRGJqjCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parada ordenada de Spark.\n",
        "spark.stop()\n",
        "print(\"\\n\")\n",
        "print(\"Spark detenido.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1nSs0jiqmXh",
        "outputId": "478b0106-4137-4dd9-9439-2a8474dba1ed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Spark detenido.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}