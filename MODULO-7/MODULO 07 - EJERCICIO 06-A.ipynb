{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQuVY5kNjX5N"
      },
      "source": [
        "\n",
        "#\n",
        "# MODULO 07 - EJERCICIO 06-A\n",
        "# ALEXIS YURI M.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6GyNyFD-1dd"
      },
      "source": [
        "Primero es necesario instalar y configurar PySpark en el notebook de Colab. Para eso se ejecutan las siguientes celdas para instalar las librerías necesarias y crear un contexto de Spark.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXX0LdusjgpU",
        "outputId": "50004975-6360-4002-ab11-2ca3820bae5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.12/dist-packages (2.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "SparkSession está lista para usar.\n"
          ]
        }
      ],
      "source": [
        "# Se instala PySpark y findspark\n",
        "!pip install pyspark findspark\n",
        "\n",
        "# Se inicializa findspark para encontrar la instalación de Spark.\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Se importa SparkContext.\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Se crea una sesión de Spark si no existe.\n",
        "try:\n",
        "    spark.stop()\n",
        "except:\n",
        "    pass\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Pipeline_mlib\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(\"SparkSession está lista para usar.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjmOAJsJkRNR"
      },
      "source": [
        "Paso 1: Lee un CSV distribuido usando spark.read.csv() y explora su estructura.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "NGblWDt5jsHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e3d523-6b1b-4a3a-8b83-c652192e600e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- sexo: string (nullable = true)\n",
            " |-- edad: long (nullable = true)\n",
            " |-- etiqueta: long (nullable = true)\n",
            "\n",
            "+----+----+--------+\n",
            "|sexo|edad|etiqueta|\n",
            "+----+----+--------+\n",
            "|   F|  25|       1|\n",
            "|   M|  30|       0|\n",
            "|   F|  35|       1|\n",
            "|   M|  40|       0|\n",
            "|   F|  45|       1|\n",
            "|   M|  50|       0|\n",
            "+----+----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Se crea un DataFrame de ejemplo simulando un archivo cvs.\n",
        "data = spark.createDataFrame([\n",
        "    (\"F\", 25, 1),\n",
        "    (\"M\", 30, 0),\n",
        "    (\"F\", 35, 1),\n",
        "    (\"M\", 40, 0),\n",
        "    (\"F\", 45, 1),\n",
        "    (\"M\", 50, 0)\n",
        "], [\"sexo\", \"edad\", \"etiqueta\"])\n",
        "\n",
        "\n",
        "# Se muestra la estructura del DataFrame.\n",
        "data.printSchema()\n",
        "data.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf4LTmNVjyzc"
      },
      "source": [
        "Paso 2: Aplica StringIndexer para convertir variables categóricas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeX_Rs9Kj4xF",
        "outputId": "b6065a32-7c27-45a9-8542-9c2e041cc882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame después de la indexación:\n",
            "+----+----+--------+---------+\n",
            "|sexo|edad|etiqueta|sexoIndex|\n",
            "+----+----+--------+---------+\n",
            "|   F|  25|       1|      0.0|\n",
            "|   M|  30|       0|      1.0|\n",
            "|   F|  35|       1|      0.0|\n",
            "|   M|  40|       0|      1.0|\n",
            "|   F|  45|       1|      0.0|\n",
            "|   M|  50|       0|      1.0|\n",
            "+----+----+--------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Se importa la librería StringIndexer.\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Se indexa la columna 'sexo' y crea una nueva columna 'sexoIndex'.\n",
        "indexer = StringIndexer(inputCol=\"sexo\", outputCol=\"sexoIndex\")\n",
        "indexed_data = indexer.fit(data).transform(data)\n",
        "\n",
        "print(\"DataFrame después de la indexación:\")\n",
        "indexed_data.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghezmLjh0JPy"
      },
      "source": [
        "Paso 3: Usa VectorAssembler para combinar las columnas de entrada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ppI_t90fgl",
        "outputId": "3f62997d-0820-41c1-c5e3-48cf39e20149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame con la columna 'features' ensamblada:\n",
            "+----+----+--------+---------+----------+\n",
            "|sexo|edad|etiqueta|sexoIndex|features  |\n",
            "+----+----+--------+---------+----------+\n",
            "|F   |25  |1       |0.0      |[0.0,25.0]|\n",
            "|M   |30  |0       |1.0      |[1.0,30.0]|\n",
            "|F   |35  |1       |0.0      |[0.0,35.0]|\n",
            "|M   |40  |0       |1.0      |[1.0,40.0]|\n",
            "|F   |45  |1       |0.0      |[0.0,45.0]|\n",
            "|M   |50  |0       |1.0      |[1.0,50.0]|\n",
            "+----+----+--------+---------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Se importa la librería.\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Se combinan las columnas de entrada en una sola columna 'features'.\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"sexoIndex\", \"edad\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "assembled_data = assembler.transform(indexed_data)\n",
        "\n",
        "print(\"DataFrame con la columna 'features' ensamblada:\")\n",
        "assembled_data.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph73fqUqj_NR"
      },
      "source": [
        "\n",
        "Paso 4: Define el modelo de clasificación (LogisticRegression)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "K1rtWg1pNbMs"
      },
      "outputs": [],
      "source": [
        "# Se importa la librería del modelo.\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Se define el modelo\n",
        "lr = LogisticRegression(labelCol=\"etiqueta\", featuresCol=\"features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SZpciMZ-u7k"
      },
      "source": [
        "Paso 5: Encadena las etapas en un Pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "CD-nI0Xj-u7l"
      },
      "outputs": [],
      "source": [
        "# Se importa la librería del pipeline.\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Se encadenan las etapas en un Pipeline.\n",
        "pipeline = Pipeline(stages=[indexer, assembler, lr])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcML8s3g-u7l"
      },
      "source": [
        "Paso 6: Divide en train y test, y entrena el modelo con .fit()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSD8sOL4-u7m",
        "outputId": "a3e74801-87fa-4459-fbed-13dc3801bc74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline entrenado con éxito.\n"
          ]
        }
      ],
      "source": [
        "# Se dividen los datos 80/20 y se entrena el modelo\n",
        "(train_df, test_df) = data.randomSplit([0.8, 0.2], seed=42)\n",
        "pipeline_model = pipeline.fit(train_df)\n",
        "\n",
        "print(\"Pipeline entrenado con éxito.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPIMc2ZQ-u7m"
      },
      "source": [
        "Paso 7: Evalúa con BinaryClassificationEvaluator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKPzCFFE-u7n",
        "outputId": "569e08a7-739e-4787-b370-ee1a8cf3b8c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicciones en el conjunto de prueba:\n",
            "+--------+----------+--------------------+\n",
            "|etiqueta|prediction|         probability|\n",
            "+--------+----------+--------------------+\n",
            "|       0|       0.0|[0.99999967348773...|\n",
            "|       1|       1.0|[3.26512258673585...|\n",
            "+--------+----------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Se mportan las librerías necesarias para la evaluación.\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "\n",
        "# Se realizan predicciones en el conjunto de prueba.\n",
        "predictions = pipeline_model.transform(test_df)\n",
        "\n",
        "# Se muestran algunas predicciones para verificar el resultado.\n",
        "print(\"Predicciones en el conjunto de prueba:\")\n",
        "predictions.select(\"etiqueta\", \"prediction\", \"probability\").show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZESTrg4-u7o"
      },
      "source": [
        "Paso 7: Muestra resultados e interpreta las métricas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfD-gEPW-u7o",
        "outputId": "d67d7358-a6fd-471c-bb67-7d173c3aec6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resultados de la evaluación:\n",
            "Área Bajo la Curva (AUC): 1.0000\n",
            "F1-score: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Se evalua con BinaryClassificationEvaluator para el AUC.\n",
        "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"etiqueta\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = binary_evaluator.evaluate(predictions)\n",
        "\n",
        "# Se evalua con MulticlassClassificationEvaluator para el F1-score.\n",
        "multiclass_evaluator = MulticlassClassificationEvaluator(labelCol=\"etiqueta\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = multiclass_evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"\\nResultados de la evaluación:\")\n",
        "print(f\"Área Bajo la Curva (AUC): {auc:.4f}\")\n",
        "print(f\"F1-score: {f1_score:.4f}\")\n",
        "\n",
        "# Se detiene la sesión de Spark.\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCfhZlyJ-u7p"
      },
      "source": [
        "Para el ejercicio, se evalúa la capacidad del modelo para distinguir entre dos clases.\n",
        "\n",
        "Las dos métricas principales obtenidas son el Área Bajo la Curva (AUC) y el F1-score.\n",
        "\n",
        "- AUC (Area Under the ROC Curve): Mide la capacidad del modelo para clasificar correctamente los ejemplos positivos y negativos. Un valor de AUC de 1.0 es perfecto, mientras que 0.5 es un rendimiento aleatorio. Cuanto más cerca de 1.0, mejor.\n",
        "\n",
        "- F1-score: Es un promedio de la precisión y el recall del modelo. Es muy útil cuando las clases en el conjunto de datos no están balanceadas. Un valor de F1-score de 1.0 indica un modelo perfecto, mientras que 0.0 indica que es ineficaz. Un valor cercano a 1.0 es excelente.\n",
        "\n",
        "En resumen, valores altos en ambas métricas (AUC y F1-score) demuestran que el modelo de clasificación tiene un alto rendimiento y es fiable en su tarea."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}