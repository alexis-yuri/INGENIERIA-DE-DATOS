{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQuVY5kNjX5N"
      },
      "source": [
        "\n",
        "#\n",
        "# MODULO 07 - EJERCICIO 03-B\n",
        "# ALEXIS YURI M.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6GyNyFD-1dd"
      },
      "source": [
        "Primero es necesario instalar y configurar PySpark en el notebook de Colab. Para eso se ejecutan las siguientes celdas para instalar las librerías necesarias y crear un contexto de Spark.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXX0LdusjgpU",
        "outputId": "1b45a730-53ea-4244-a061-b95176ca75a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "SparkSession está lista para usar.\n"
          ]
        }
      ],
      "source": [
        "# Se instala PySpark y findspark\n",
        "!pip install pyspark findspark\n",
        "\n",
        "# Se inicializa findspark para encontrar la instalación de Spark.\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Se importa SparkContext.\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Se crea una sesión de Spark si no existe.\n",
        "try:\n",
        "    spark.stop()\n",
        "except:\n",
        "    pass\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"EjecutarJob\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(\"SparkSession está lista para usar.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjmOAJsJkRNR"
      },
      "source": [
        "Paso 1: Se crea un RDD a partir de una lista de números. Se utilizará la API SparkContext.parallelize() para distribuir la lista a través de los nodos de un clúster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGblWDt5jsHk",
        "outputId": "b1174d39-f603-458e-c3be-3bd622d411da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El RDD creado es: ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289\n",
            "Número de particiones lógicas del RDD: 2\n"
          ]
        }
      ],
      "source": [
        "# Se crea una lista de datos simulados.\n",
        "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "# Se carga la lista en un RDD.\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "print(f\"El RDD creado es: {rdd}\")\n",
        "print(f\"Número de particiones lógicas del RDD: {rdd.getNumPartitions()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf4LTmNVjyzc"
      },
      "source": [
        "Paso 2: Se aplican transformaciones.\n",
        "\n",
        "Las transformaciones en Spark, como filter y map, son lazy. Esto significa que no se ejecutan de inmediato; en su lugar, se construye un plan de ejecución (el DAG)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GeX_Rs9Kj4xF"
      },
      "outputs": [],
      "source": [
        "# Se aplica la transformación \"filter\".\n",
        "rdd_filtered = rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Se aplica la transformación \"map\".\n",
        "rdd_mapped = rdd_filtered.map(lambda x: x * 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghezmLjh0JPy"
      },
      "source": [
        "Paso 3: Se confirma que aú no se ha ejecutado nada (Lazy Evaluation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ppI_t90fgl",
        "outputId": "ed87efa6-3f9d-48ac-984d-a0295c9b688f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El plan de ejecución (DAG) se ha construido, pero aún no se ha ejecutado ninguna operación.\n",
            "Resultado después de aplicar 'filter' al RDD: PythonRDD[1] at RDD at PythonRDD.scala:53\n",
            "Resultado después de aplicar 'map' al RDD: PythonRDD[2] at RDD at PythonRDD.scala:53\n"
          ]
        }
      ],
      "source": [
        "# Se muestra el estado del RDD.\n",
        "\n",
        "print(\"El plan de ejecución (DAG) se ha construido, pero aún no se ha ejecutado ninguna operación.\")\n",
        "print(f\"Resultado después de aplicar 'filter' al RDD: {rdd_filtered}\")\n",
        "print(f\"Resultado después de aplicar 'map' al RDD: {rdd_mapped}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph73fqUqj_NR"
      },
      "source": [
        "\n",
        "Paso 4: Ejecutar una acción. Spark activa el plan de ejecución para producir un resultado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr3RIs9BkEzp",
        "outputId": "8fa28771-8b08-4813-99a5-ceb380bd7a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Se ha llamado la acción 'mean()'. Esto dispara la ejecución del Job.\n",
            "El valor medio de los datos transformados es: 60.0\n",
            "\n",
            "\n",
            "Spark detenido.\n"
          ]
        }
      ],
      "source": [
        "# Se ejecuta la acción \"mean()\".\n",
        "resultado = rdd_mapped.mean()\n",
        "\n",
        "print(f\"\\nSe ha llamado la acción 'mean()'. Esto dispara la ejecución del Job.\")\n",
        "print(f\"El valor medio de los datos transformados es: {resultado}\")\n",
        "\n",
        "\n",
        "# Parada ordenada de Spark.\n",
        "spark.stop()\n",
        "print(\"\\n\")\n",
        "print(\"Spark detenido.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRYQBE0OkIfD"
      },
      "source": [
        "Paso 5:. Visualizar el Job: DAG, Stages y Tasks\n",
        "\n",
        "Cuando la acción mean() se ejecuta, el SparkContext crea un Job. Este Job se descompone en Stages, y cada Stage se divide en Tasks.\n",
        "\n",
        "  DAG (Directed Acyclic Graph): Es el plan de ejecución del Job. Representa las dependencias entre las transformaciones y la acción final. Para este ejercicio, el DAG se vería de la siguiente manera:\n",
        "\n",
        "  parallelize -> filter -> map -> mean\n",
        "\n",
        "  Stages: El DAG se divide en Stages en función de las operaciones que requieren mover datos entre los nodos del clúster (operaciones de shuffle). En este caso, probablemente haya un solo Stage, ya que filter y map no requieren un shuffle.\n",
        "\n",
        "  Tasks: Cada Stage se divide en tareas. Una tarea es la unidad de trabajo más pequeña que se envía a un executor (un proceso de trabajo) para que se ejecute en una partición de datos. Si el RDD tiene 4 particiones, habrá 4 tareas para el Stage de filter y 4 para el de map.\n",
        "\n",
        "\n",
        "------------------------------------------------\n",
        "\n",
        "Paso 6: Diagrama conceptual del DAG.\n",
        "\n",
        "graph TD\n",
        "    A[Lista de números] --> B(RDD)\n",
        "    B --> C{filter: x % 2 == 0};\n",
        "    C --> D[map: x * 10];\n",
        "    D --> E[mean(): acción];\n",
        "    E --> F[Resultado Final: 60.0];\n",
        "\n",
        "Explicación:\n",
        "\n",
        "  Stage 1: Inicia con la lectura de los datos.\n",
        "\n",
        "  Task 1: Procesa la partición 1 (e.g., [1, 2, 3]) -> Resultado: [2] -> [20]\n",
        "\n",
        "  Task 2: Procesa la partición 2 (e.g., [4, 5, 6]) -> Resultado: [4, 6] -> [40, 60]\n",
        "\n",
        "  Task 3: Procesa la partición 3 (e.g., [7, 8, 9]) -> Resultado: [8] -> [80]\n",
        "\n",
        "  Task 4: Procesa la partición 4 (e.g., [10]) -> Resultado: [10] -> [100]\n",
        "\n",
        "  Acción mean(): El driver Spark recolecta los resultados parciales de cada tarea ([20, 40, 60, 80, 100]) y calcula el promedio final.\n",
        "\n",
        "El resultado final, 60.0, es la media de los números pares transformados (2, 4, 6, 8, 10), que al ser multiplicados por 10 son 20, 40, 60, 80 y 100.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}