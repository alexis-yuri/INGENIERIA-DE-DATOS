{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQuVY5kNjX5N"
      },
      "source": [
        "\n",
        "#\n",
        "# MODULO 07 - EJERCICIO 04-A\n",
        "# ALEXIS YURI M.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6GyNyFD-1dd"
      },
      "source": [
        "Primero es necesario instalar y configurar PySpark en el notebook de Colab. Para eso se ejecutan las siguientes celdas para instalar las librerías necesarias y crear un contexto de Spark.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXX0LdusjgpU",
        "outputId": "17be7a55-2630-442e-ae5b-fa8f7bd4ec93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "SparkSession está lista para usar.\n"
          ]
        }
      ],
      "source": [
        "# Se instala PySpark y findspark\n",
        "!pip install pyspark findspark\n",
        "\n",
        "# Se inicializa findspark para encontrar la instalación de Spark.\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Se importa SparkContext.\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Se crea una sesión de Spark si no existe.\n",
        "try:\n",
        "    spark.stop()\n",
        "except:\n",
        "    pass\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"AnalisisAgricola\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(\"SparkSession está lista para usar.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjmOAJsJkRNR"
      },
      "source": [
        "Paso 1: Se carga el archivo CSV, para eso se debe subir el archivo 1. datos_agricolas.csv en el panel de archivos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NGblWDt5jsHk"
      },
      "outputs": [],
      "source": [
        "# Se carga el archivo CSV en un DataFrame.\n",
        "df_agricola = spark.read.csv('1. datos_agricolas.csv', header=True, sep=';', inferSchema=True , encoding='utf-8')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf4LTmNVjyzc"
      },
      "source": [
        "Paso 2: Se muestran las primera filas y se analiza la estructura del Dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeX_Rs9Kj4xF",
        "outputId": "6132cd0f-00ff-4ce3-aef3-f01f4d872327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esquema del DataFrame:\n",
            "root\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Mes: integer (nullable = true)\n",
            " |-- Region: string (nullable = true)\n",
            " |-- Cultivo: string (nullable = true)\n",
            " |-- Superficie_Ha: integer (nullable = true)\n",
            " |-- Produccion_Ton: integer (nullable = true)\n",
            " |-- Precio_Ton: integer (nullable = true)\n",
            " |-- Costos_Insumos: integer (nullable = true)\n",
            " |-- Ingresos: integer (nullable = true)\n",
            " |-- Utilidad: integer (nullable = true)\n",
            "\n",
            "\n",
            "Primeras 5 filas:\n",
            "+----+---+------+-------+-------------+--------------+----------+--------------+--------+--------+\n",
            "|Year|Mes|Region|Cultivo|Superficie_Ha|Produccion_Ton|Precio_Ton|Costos_Insumos|Ingresos|Utilidad|\n",
            "+----+---+------+-------+-------------+--------------+----------+--------------+--------+--------+\n",
            "|2023|  7|   Sur|Manzana|           34|          4583|       378|          5267| 1730770| 1725503|\n",
            "|2024|  5|Centro| Tomate|          253|          3501|       106|          6988|  372681|  365693|\n",
            "|2023|  3|   Sur|   Soja|          430|           716|       249|          9680|  178327|  168647|\n",
            "|2023|  3|   Sur|   Soja|           37|          3074|       241|          2485|  739850|  737365|\n",
            "|2023|  4| Norte|   Ma�z|          462|           763|       317|         15713|  242169|  226456|\n",
            "+----+---+------+-------+-------------+--------------+----------+--------------+--------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Resumen del Dataframe:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, Year: string, Mes: string, Region: string, Cultivo: string, Superficie_Ha: string, Produccion_Ton: string, Precio_Ton: string, Costos_Insumos: string, Ingresos: string, Utilidad: string]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Se muestran las primeras filas, los tipos de datos y el esquema para confirmar que la carga fue exitosa.\n",
        "print(\"Esquema del DataFrame:\")\n",
        "df_agricola.printSchema()\n",
        "print(\"\\nPrimeras 5 filas:\")\n",
        "df_agricola.show(5)\n",
        "print(\"\\nResumen del Dataframe:\")\n",
        "df_agricola.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghezmLjh0JPy"
      },
      "source": [
        "Paso 3: Se registra Dataframe como vista temporal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ppI_t90fgl",
        "outputId": "1f7f808a-9bba-41c0-bc78-da5b53c0ca54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vista temporal registrada.\n"
          ]
        }
      ],
      "source": [
        "# Se crea una vista temporal para poder usar Spark SQL.\n",
        "df_agricola.createOrReplaceTempView(\"datos_agricolas\")\n",
        "\n",
        "print(\"Vista temporal registrada.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph73fqUqj_NR"
      },
      "source": [
        "\n",
        "Paso 4: Se ejecutan consultas con Spark SQL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1rtWg1pNbMs",
        "outputId": "53809a55-b826-4ffe-848e-870b4c0d29e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendimiento promedio por región:\n",
            "+------+--------------------+\n",
            "|Region|Rendimiento_Promedio|\n",
            "+------+--------------------+\n",
            "|   Sur|   23.34617487434811|\n",
            "| Norte|  27.802923541185088|\n",
            "|Centro|  25.610502052637127|\n",
            "+------+--------------------+\n",
            "\n",
            "Top 3 cultivos más rentables:\n",
            "+-------+--------------+\n",
            "|Cultivo|Utilidad_Total|\n",
            "+-------+--------------+\n",
            "|Manzana|     206003764|\n",
            "| Tomate|     195542018|\n",
            "|    Uva|     193713685|\n",
            "+-------+--------------+\n",
            "\n",
            "Ingresos totales para la Papa en la región Norte en 2023:\n",
            "+------------------------+\n",
            "|Ingresos_Papa_Norte_2023|\n",
            "+------------------------+\n",
            "|                35488409|\n",
            "+------------------------+\n",
            "\n",
            "\n",
            "\n",
            "Spark detenido.\n"
          ]
        }
      ],
      "source": [
        "# Consulta 1: Rendimiento promedio por región.\n",
        "# El rendimiento promedio se calcula como Producción en Toneladas dividido por Superficie en Hectáreas.\n",
        "# Se agrupa el resultado por región.\n",
        "\n",
        "\n",
        "print(\"Rendimiento promedio por región:\")\n",
        "\n",
        "consulta1=spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        Region,\n",
        "        AVG(Produccion_Ton / Superficie_Ha) AS Rendimiento_Promedio\n",
        "    FROM\n",
        "        datos_agricolas\n",
        "    GROUP BY\n",
        "        Region\n",
        "\"\"\")\n",
        "consulta1.show()\n",
        "\n",
        "\n",
        "# Consulta 2: Top 3 cultivos más rentables.\n",
        "# La rentabilidad se mide por la suma total de Utilidad.\n",
        "# Se agrupa por cultivo y se muestran los 3 mayores en orden descendente.\n",
        "\n",
        "print(\"Top 3 cultivos más rentables:\")\n",
        "consulta2=spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        Cultivo,\n",
        "        SUM(Utilidad) AS Utilidad_Total\n",
        "    FROM\n",
        "        datos_agricolas\n",
        "    GROUP BY\n",
        "        Cultivo\n",
        "    ORDER BY\n",
        "        Utilidad_Total DESC\n",
        "    LIMIT 3\n",
        "\"\"\")\n",
        "consulta2.show()\n",
        "\n",
        "\n",
        "# Consulta 3: Ingresos totales para la Papa en la región Norte en el año 2023.\n",
        "# Se suman todos los ingresos del año 2023 en la region Norte para el Cultivo de la Papa.\n",
        "\n",
        "print(\"Ingresos totales para la Papa en la región Norte en 2023:\")\n",
        "consulta3=spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        SUM(Ingresos) AS Ingresos_Papa_Norte_2023\n",
        "    FROM\n",
        "        datos_agricolas\n",
        "    WHERE\n",
        "        Year = 2023 AND Region = \"Norte\" AND Cultivo = \"Papa\"\n",
        "\"\"\")\n",
        "consulta3.show()\n",
        "\n",
        "\n",
        "\n",
        "# Parada ordenada de Spark.\n",
        "spark.stop()\n",
        "print(\"\\n\")\n",
        "print(\"Spark detenido.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}